{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import scipy, multiprocessing\n",
    "import tensorflow as tf\n",
    "import tensorlayer as tl\n",
    "from model import get_G, get_D\n",
    "from config import config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "###====================== HYPER-PARAMETERS ===========================###\n",
    "\n",
    "batch_size = config.TRAIN.batch_size  # use 8 if your GPU memory is small, and change [4, 4] in tl.vis.save_images to [2, 4]\n",
    "lr_init = config.TRAIN.lr_init\n",
    "beta1 = config.TRAIN.beta1\n",
    "## initialize G\n",
    "n_epoch_init = config.TRAIN.n_epoch_init\n",
    "\n",
    "n_epoch = config.TRAIN.n_epoch\n",
    "lr_decay = config.TRAIN.lr_decay\n",
    "decay_every = config.TRAIN.decay_every\n",
    "shuffle_buffer_size = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TL] [!] samples exists ...\n",
      "[TL] [!] checkpoint exists ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create folders to save result images and trained models\n",
    "save_dir = \"samples\"\n",
    "tl.files.exists_or_mkdir(save_dir)\n",
    "checkpoint_dir = \"checkpoint\"\n",
    "tl.files.exists_or_mkdir(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data():\n",
    "    # load dataset\n",
    "#     train_hr_img_list = sorted(tl.files.load_file_list(path=config.TRAIN.hr_img_path, regx='.*.png', printable=False))[:3000]\n",
    "    train_lr_img_list = sorted(tl.files.load_file_list(path='DIV2K/DIV2K_train_LR_wild/', regx='.*.png', printable=False))\n",
    "\n",
    "    ## If your machine have enough memory, please pre-load the entire train set.\n",
    "#     train_hr_imgs = tl.vis.read_images(train_hr_img_list, path=config.TRAIN.hr_img_path, n_threads=32)\n",
    "    train_lr_imgs = tl.vis.read_images(train_lr_img_list, path='DIV2K/DIV2K_train_LR_wild/', n_threads=32)\n",
    "        \n",
    "    # dataset API and augmentation\n",
    "    def generator_train():\n",
    "        for img in train_lr_imgs:\n",
    "            yield img\n",
    "            \n",
    "    def _map_fn_train(img):\n",
    "#         hr_patch = tf.image.random_crop(img, [384, 384, 3])\n",
    "#         hr_patch = hr_patch / (255. / 2.)\n",
    "#         hr_patch = hr_patch - 1.\n",
    "#         hr_patch = tf.image.random_flip_left_right(hr_patch)\n",
    "        \n",
    "        lr_patch = tf.image.random_crop(img, [96, 96, 3])\n",
    "        lr_patch = lr_patch / (255. / 2.)\n",
    "        lr_patch = lr_patch - 1.\n",
    "        lr_patch = tf.image.random_flip_left_right(lr_patch)\n",
    "    \n",
    "        return lr_patch\n",
    "    \n",
    "    train_ds = tf.data.Dataset.from_generator(generator_train, output_types=(tf.float32))\n",
    "    train_ds = train_ds.map(_map_fn_train, num_parallel_calls=multiprocessing.cpu_count())\n",
    "        # train_ds = train_ds.repeat(n_epoch_init + n_epoch)\n",
    "    train_ds = train_ds.shuffle(shuffle_buffer_size)\n",
    "    train_ds = train_ds.prefetch(buffer_size=2)\n",
    "    train_ds = train_ds.batch(batch_size)\n",
    "        # value = train_ds.make_one_shot_iterator().get_next()\n",
    "    return train_ds\n",
    "\n",
    "def downscale_hr_patches(hr_patch):\n",
    "    return tf.image.resize(hr_patch, size=[96, 96])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TL] Input  _inputlayer_1: (8, 96, 96, 3)\n",
      "[TL] Conv2d conv2d_1: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: relu\n",
      "[TL] Conv2d conv2d_2: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_1: decay: 0.900000 epsilon: 0.000010 act: relu is_train: False\n",
      "[TL] Conv2d conv2d_3: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_2: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: False\n",
      "[TL] Elementwise elementwise_1: fn: add act: No Activation\n",
      "[TL] Conv2d conv2d_4: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_3: decay: 0.900000 epsilon: 0.000010 act: relu is_train: False\n",
      "[TL] Conv2d conv2d_5: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_4: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: False\n",
      "[TL] Elementwise elementwise_2: fn: add act: No Activation\n",
      "[TL] Conv2d conv2d_6: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_5: decay: 0.900000 epsilon: 0.000010 act: relu is_train: False\n",
      "[TL] Conv2d conv2d_7: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_6: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: False\n",
      "[TL] Elementwise elementwise_3: fn: add act: No Activation\n",
      "[TL] Conv2d conv2d_8: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_7: decay: 0.900000 epsilon: 0.000010 act: relu is_train: False\n",
      "[TL] Conv2d conv2d_9: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_8: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: False\n",
      "[TL] Elementwise elementwise_4: fn: add act: No Activation\n",
      "[TL] Conv2d conv2d_10: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_9: decay: 0.900000 epsilon: 0.000010 act: relu is_train: False\n",
      "[TL] Conv2d conv2d_11: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_10: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: False\n",
      "[TL] Elementwise elementwise_5: fn: add act: No Activation\n",
      "[TL] Conv2d conv2d_12: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_11: decay: 0.900000 epsilon: 0.000010 act: relu is_train: False\n",
      "[TL] Conv2d conv2d_13: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_12: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: False\n",
      "[TL] Elementwise elementwise_6: fn: add act: No Activation\n",
      "[TL] Conv2d conv2d_14: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_13: decay: 0.900000 epsilon: 0.000010 act: relu is_train: False\n",
      "[TL] Conv2d conv2d_15: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_14: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: False\n",
      "[TL] Elementwise elementwise_7: fn: add act: No Activation\n",
      "[TL] Conv2d conv2d_16: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_15: decay: 0.900000 epsilon: 0.000010 act: relu is_train: False\n",
      "[TL] Conv2d conv2d_17: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_16: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: False\n",
      "[TL] Elementwise elementwise_8: fn: add act: No Activation\n",
      "[TL] Conv2d conv2d_18: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_17: decay: 0.900000 epsilon: 0.000010 act: relu is_train: False\n",
      "[TL] Conv2d conv2d_19: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_18: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: False\n",
      "[TL] Elementwise elementwise_9: fn: add act: No Activation\n",
      "[TL] Conv2d conv2d_20: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_19: decay: 0.900000 epsilon: 0.000010 act: relu is_train: False\n",
      "[TL] Conv2d conv2d_21: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_20: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: False\n",
      "[TL] Elementwise elementwise_10: fn: add act: No Activation\n",
      "[TL] Conv2d conv2d_22: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_21: decay: 0.900000 epsilon: 0.000010 act: relu is_train: False\n",
      "[TL] Conv2d conv2d_23: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_22: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: False\n",
      "[TL] Elementwise elementwise_11: fn: add act: No Activation\n",
      "[TL] Conv2d conv2d_24: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_23: decay: 0.900000 epsilon: 0.000010 act: relu is_train: False\n",
      "[TL] Conv2d conv2d_25: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_24: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: False\n",
      "[TL] Elementwise elementwise_12: fn: add act: No Activation\n",
      "[TL] Conv2d conv2d_26: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_25: decay: 0.900000 epsilon: 0.000010 act: relu is_train: False\n",
      "[TL] Conv2d conv2d_27: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_26: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: False\n",
      "[TL] Elementwise elementwise_13: fn: add act: No Activation\n",
      "[TL] Conv2d conv2d_28: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_27: decay: 0.900000 epsilon: 0.000010 act: relu is_train: False\n",
      "[TL] Conv2d conv2d_29: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_28: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: False\n",
      "[TL] Elementwise elementwise_14: fn: add act: No Activation\n",
      "[TL] Conv2d conv2d_30: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_29: decay: 0.900000 epsilon: 0.000010 act: relu is_train: False\n",
      "[TL] Conv2d conv2d_31: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_30: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: False\n",
      "[TL] Elementwise elementwise_15: fn: add act: No Activation\n",
      "[TL] Conv2d conv2d_32: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_31: decay: 0.900000 epsilon: 0.000010 act: relu is_train: False\n",
      "[TL] Conv2d conv2d_33: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_32: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: False\n",
      "[TL] Elementwise elementwise_16: fn: add act: No Activation\n",
      "[TL] Conv2d conv2d_34: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_33: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: False\n",
      "[TL] Elementwise elementwise_17: fn: add act: No Activation\n",
      "[TL] Conv2d conv2d_35: n_filter: 256 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] SubpixelConv2d  subpixelconv2d_1: scale: 2 act: relu\n",
      "[TL] Conv2d conv2d_36: n_filter: 256 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] SubpixelConv2d  subpixelconv2d_2: scale: 2 act: relu\n",
      "[TL] Conv2d conv2d_37: n_filter: 3 filter_size: (1, 1) strides: (1, 1) pad: SAME act: tanh\n",
      "[TL] Input  _inputlayer_2: (8, 384, 384, 3)\n",
      "[TL] Conv2d conv2d_38: n_filter: 64 filter_size: (4, 4) strides: (2, 2) pad: SAME act: <lambda>\n",
      "[TL] Conv2d conv2d_39: n_filter: 128 filter_size: (4, 4) strides: (2, 2) pad: SAME act: No Activation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TL] BatchNorm batchnorm2d_34: decay: 0.900000 epsilon: 0.000010 act: <lambda> is_train: False\n",
      "[TL] Conv2d conv2d_40: n_filter: 256 filter_size: (4, 4) strides: (2, 2) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_35: decay: 0.900000 epsilon: 0.000010 act: <lambda> is_train: False\n",
      "[TL] Conv2d conv2d_41: n_filter: 512 filter_size: (4, 4) strides: (2, 2) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_36: decay: 0.900000 epsilon: 0.000010 act: <lambda> is_train: False\n",
      "[TL] Conv2d conv2d_42: n_filter: 1024 filter_size: (4, 4) strides: (2, 2) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_37: decay: 0.900000 epsilon: 0.000010 act: <lambda> is_train: False\n",
      "[TL] Conv2d conv2d_43: n_filter: 2048 filter_size: (4, 4) strides: (2, 2) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_38: decay: 0.900000 epsilon: 0.000010 act: <lambda> is_train: False\n",
      "[TL] Conv2d conv2d_44: n_filter: 1024 filter_size: (1, 1) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_39: decay: 0.900000 epsilon: 0.000010 act: <lambda> is_train: False\n",
      "[TL] Conv2d conv2d_45: n_filter: 512 filter_size: (1, 1) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_40: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: False\n",
      "[TL] Conv2d conv2d_46: n_filter: 128 filter_size: (1, 1) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_41: decay: 0.900000 epsilon: 0.000010 act: <lambda> is_train: False\n",
      "[TL] Conv2d conv2d_47: n_filter: 128 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_42: decay: 0.900000 epsilon: 0.000010 act: <lambda> is_train: False\n",
      "[TL] Conv2d conv2d_48: n_filter: 512 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_43: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: False\n",
      "[TL] Elementwise elementwise_18: fn: add act: <lambda>\n",
      "[TL] Flatten flatten_1:\n",
      "[TL] Dense  dense_1: 1 No Activation\n",
      "[TL] Input  _inputlayer_3: [None, 224, 224, 3]\n",
      "[TL] Lambda  scale: func: <function VGG_static.<locals>.<lambda> at 0x7f8260015f28>, len_weights: 0\n",
      "[TL] Conv2d conv1_1: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: relu\n",
      "[TL] Conv2d conv1_2: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: relu\n",
      "[TL] MaxPool2d pool1: filter_size: (2, 2) strides: (2, 2) padding: SAME\n",
      "[TL] Conv2d conv2_1: n_filter: 128 filter_size: (3, 3) strides: (1, 1) pad: SAME act: relu\n",
      "[TL] Conv2d conv2_2: n_filter: 128 filter_size: (3, 3) strides: (1, 1) pad: SAME act: relu\n",
      "[TL] MaxPool2d pool2: filter_size: (2, 2) strides: (2, 2) padding: SAME\n",
      "[TL] Conv2d conv3_1: n_filter: 256 filter_size: (3, 3) strides: (1, 1) pad: SAME act: relu\n",
      "[TL] Conv2d conv3_2: n_filter: 256 filter_size: (3, 3) strides: (1, 1) pad: SAME act: relu\n",
      "[TL] Conv2d conv3_3: n_filter: 256 filter_size: (3, 3) strides: (1, 1) pad: SAME act: relu\n",
      "[TL] Conv2d conv3_4: n_filter: 256 filter_size: (3, 3) strides: (1, 1) pad: SAME act: relu\n",
      "[TL] MaxPool2d pool3: filter_size: (2, 2) strides: (2, 2) padding: SAME\n",
      "[TL] Conv2d conv4_1: n_filter: 512 filter_size: (3, 3) strides: (1, 1) pad: SAME act: relu\n",
      "[TL] Conv2d conv4_2: n_filter: 512 filter_size: (3, 3) strides: (1, 1) pad: SAME act: relu\n",
      "[TL] Conv2d conv4_3: n_filter: 512 filter_size: (3, 3) strides: (1, 1) pad: SAME act: relu\n",
      "[TL] Conv2d conv4_4: n_filter: 512 filter_size: (3, 3) strides: (1, 1) pad: SAME act: relu\n",
      "[TL] MaxPool2d pool4: filter_size: (2, 2) strides: (2, 2) padding: SAME\n",
      "[TL] LayerList layerlist_1 including layers [conv1_1, conv1_2, pool1, conv2_1, conv2_2, pool2, conv3_1, conv3_2, conv3_3, conv3_4, pool3, conv4_1, conv4_2, conv4_3, conv4_4, pool4]\n",
      "[TL] Restore pre-trained weights\n",
      "[TL]   Loading (3, 3, 3, 64) in conv1_1\n",
      "[TL]   Loading (64,) in conv1_1\n",
      "[TL]   Loading (3, 3, 64, 64) in conv1_2\n",
      "[TL]   Loading (64,) in conv1_2\n",
      "[TL]   Loading (3, 3, 64, 128) in conv2_1\n",
      "[TL]   Loading (128,) in conv2_1\n",
      "[TL]   Loading (3, 3, 128, 128) in conv2_2\n",
      "[TL]   Loading (128,) in conv2_2\n",
      "[TL]   Loading (3, 3, 128, 256) in conv3_1\n",
      "[TL]   Loading (256,) in conv3_1\n",
      "[TL]   Loading (3, 3, 256, 256) in conv3_2\n",
      "[TL]   Loading (256,) in conv3_2\n",
      "[TL]   Loading (3, 3, 256, 256) in conv3_3\n",
      "[TL]   Loading (256,) in conv3_3\n",
      "[TL]   Loading (3, 3, 256, 256) in conv3_4\n",
      "[TL]   Loading (256,) in conv3_4\n",
      "[TL]   Loading (3, 3, 256, 512) in conv4_1\n",
      "[TL]   Loading (512,) in conv4_1\n",
      "[TL]   Loading (3, 3, 512, 512) in conv4_2\n",
      "[TL]   Loading (512,) in conv4_2\n",
      "[TL]   Loading (3, 3, 512, 512) in conv4_3\n",
      "[TL]   Loading (512,) in conv4_3\n",
      "[TL]   Loading (3, 3, 512, 512) in conv4_4\n",
      "[TL]   Loading (512,) in conv4_4\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'DIV2/DIV2K_train_LR_wild/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-60c432fed30a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mVGG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtrain_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_train_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-1cfb6b2c2fd1>\u001b[0m in \u001b[0;36mget_train_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# load dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#     train_hr_img_list = sorted(tl.files.load_file_list(path=config.TRAIN.hr_img_path, regx='.*.png', printable=False))[:3000]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain_lr_img_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_file_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'DIV2/DIV2K_train_LR_wild/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'.*.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprintable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m## If your machine have enough memory, please pre-load the entire train set.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorlayer/files/utils.py\u001b[0m in \u001b[0;36mload_file_list\u001b[0;34m(path, regx, printable, keep_prefix)\u001b[0m\n\u001b[1;32m   2341\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2342\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2343\u001b[0;31m     \u001b[0mfile_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2344\u001b[0m     \u001b[0mreturn_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2345\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'DIV2/DIV2K_train_LR_wild/'"
     ]
    }
   ],
   "source": [
    "G = get_G((batch_size, 96, 96, 3))\n",
    "D = get_D((batch_size, 384, 384, 3))\n",
    "VGG = tl.models.vgg19(pretrained=True, end_with='pool4', mode='static')\n",
    "\n",
    "lr_v = tf.Variable(lr_init)\n",
    "g_optimizer_init = tf.optimizers.Adam(lr_v, beta_1=beta1)\n",
    "g_optimizer = tf.optimizers.Adam(lr_v, beta_1=beta1)\n",
    "d_optimizer = tf.optimizers.Adam(lr_v, beta_1=beta1)\n",
    "\n",
    "G.train()\n",
    "D.train()\n",
    "VGG.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TL] read 32 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 64 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 96 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 128 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 160 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 192 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 224 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 256 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 288 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 320 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 352 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 384 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 416 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 448 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 480 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 512 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 544 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 576 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 608 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 640 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 672 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 704 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 736 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 768 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 800 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 832 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 864 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 896 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 928 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 960 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 992 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 1024 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 1056 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 1088 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 1120 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 1152 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 1184 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 1216 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 1248 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 1280 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 1312 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 1344 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 1376 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 1408 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 1440 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 1472 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 1504 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 1536 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 1568 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 1600 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 1632 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 1664 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 1696 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 1728 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 1760 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 1792 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 1824 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 1856 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 1888 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 1920 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 1952 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 1984 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 2016 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 2048 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 2080 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 2112 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 2144 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 2176 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 2208 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 2240 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 2272 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 2304 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 2336 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 2368 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 2400 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 2432 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 2464 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 2496 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 2528 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 2560 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 2592 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 2624 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 2656 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 2688 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 2720 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 2752 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 2784 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 2816 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 2848 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 2880 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 2976 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 3008 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 3040 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 3072 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 3104 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 3136 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 3168 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 3200 from DIV2K/DIV2K_train_LR_wild/\n"
     ]
    }
   ],
   "source": [
    "train_ds = get_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/100] step: [0/12] time: 9.455s, mse: 0.216 \n",
      "Epoch: [0/100] step: [1/12] time: 9.256s, mse: 0.289 \n",
      "Epoch: [0/100] step: [2/12] time: 9.070s, mse: 0.237 \n",
      "Epoch: [0/100] step: [3/12] time: 9.140s, mse: 0.130 \n",
      "Epoch: [0/100] step: [13/12] time: 9.038s, mse: 0.198 \n",
      "Epoch: [0/100] step: [14/12] time: 9.047s, mse: 0.135 \n",
      "Epoch: [0/100] step: [15/12] time: 9.033s, mse: 0.236 \n",
      "Epoch: [0/100] step: [16/12] time: 9.127s, mse: 0.113 \n",
      "Epoch: [0/100] step: [17/12] time: 9.315s, mse: 0.131 \n",
      "Epoch: [0/100] step: [18/12] time: 9.127s, mse: 0.125 \n",
      "Epoch: [0/100] step: [19/12] time: 9.122s, mse: 0.135 \n",
      "Epoch: [0/100] step: [20/12] time: 9.157s, mse: 0.111 \n",
      "Epoch: [0/100] step: [21/12] time: 9.040s, mse: 0.096 \n",
      "Epoch: [0/100] step: [22/12] time: 9.039s, mse: 0.077 \n",
      "Epoch: [0/100] step: [23/12] time: 9.358s, mse: 0.131 \n",
      "Epoch: [0/100] step: [24/12] time: 9.189s, mse: 0.055 \n",
      "Epoch: [0/100] step: [25/12] time: 9.067s, mse: 0.065 \n",
      "Epoch: [0/100] step: [26/12] time: 9.068s, mse: 0.078 \n",
      "Epoch: [0/100] step: [27/12] time: 9.093s, mse: 0.052 \n",
      "Epoch: [0/100] step: [28/12] time: 9.074s, mse: 0.085 \n",
      "Epoch: [0/100] step: [29/12] time: 9.003s, mse: 0.069 \n",
      "Epoch: [0/100] step: [30/12] time: 9.351s, mse: 0.072 \n",
      "Epoch: [0/100] step: [31/12] time: 9.084s, mse: 0.050 \n",
      "Epoch: [0/100] step: [32/12] time: 9.054s, mse: 0.042 \n",
      "Epoch: [0/100] step: [33/12] time: 9.008s, mse: 0.054 \n",
      "Epoch: [0/100] step: [34/12] time: 8.976s, mse: 0.094 \n",
      "Epoch: [0/100] step: [35/12] time: 9.027s, mse: 0.050 \n",
      "Epoch: [0/100] step: [36/12] time: 9.023s, mse: 0.068 \n",
      "Epoch: [0/100] step: [37/12] time: 9.302s, mse: 0.047 \n",
      "Epoch: [0/100] step: [38/12] time: 8.985s, mse: 0.052 \n",
      "Epoch: [0/100] step: [39/12] time: 9.056s, mse: 0.036 \n",
      "Epoch: [0/100] step: [40/12] time: 9.130s, mse: 0.052 \n",
      "Epoch: [0/100] step: [41/12] time: 9.009s, mse: 0.068 \n",
      "Epoch: [0/100] step: [42/12] time: 8.976s, mse: 0.051 \n",
      "Epoch: [0/100] step: [43/12] time: 9.270s, mse: 0.030 \n",
      "Epoch: [0/100] step: [44/12] time: 9.064s, mse: 0.057 \n",
      "Epoch: [0/100] step: [45/12] time: 9.061s, mse: 0.034 \n",
      "Epoch: [0/100] step: [46/12] time: 9.064s, mse: 0.034 \n",
      "Epoch: [0/100] step: [47/12] time: 9.059s, mse: 0.034 \n",
      "Epoch: [0/100] step: [48/12] time: 8.954s, mse: 0.047 \n",
      "Epoch: [0/100] step: [49/12] time: 9.091s, mse: 0.043 \n",
      "Epoch: [0/100] step: [50/12] time: 9.223s, mse: 0.042 \n",
      "Epoch: [0/100] step: [51/12] time: 9.060s, mse: 0.054 \n",
      "Epoch: [0/100] step: [52/12] time: 9.001s, mse: 0.033 \n",
      "Epoch: [0/100] step: [53/12] time: 8.993s, mse: 0.036 \n",
      "Epoch: [0/100] step: [54/12] time: 9.026s, mse: 0.030 \n",
      "Epoch: [0/100] step: [55/12] time: 9.018s, mse: 0.020 \n",
      "Epoch: [0/100] step: [56/12] time: 9.265s, mse: 0.049 \n",
      "Epoch: [0/100] step: [57/12] time: 9.045s, mse: 0.049 \n",
      "Epoch: [0/100] step: [58/12] time: 9.029s, mse: 0.022 \n",
      "Epoch: [0/100] step: [59/12] time: 9.062s, mse: 0.045 \n",
      "Epoch: [0/100] step: [60/12] time: 9.063s, mse: 0.034 \n",
      "Epoch: [0/100] step: [61/12] time: 9.083s, mse: 0.023 \n",
      "Epoch: [0/100] step: [62/12] time: 9.009s, mse: 0.020 \n",
      "Epoch: [0/100] step: [63/12] time: 9.238s, mse: 0.027 \n",
      "Epoch: [0/100] step: [64/12] time: 9.179s, mse: 0.051 \n",
      "Epoch: [0/100] step: [65/12] time: 9.012s, mse: 0.037 \n",
      "Epoch: [0/100] step: [66/12] time: 9.040s, mse: 0.032 \n",
      "Epoch: [0/100] step: [67/12] time: 9.106s, mse: 0.037 \n",
      "Epoch: [0/100] step: [68/12] time: 9.036s, mse: 0.025 \n",
      "Epoch: [0/100] step: [69/12] time: 9.073s, mse: 0.049 \n",
      "Epoch: [0/100] step: [70/12] time: 9.322s, mse: 0.064 \n",
      "Epoch: [0/100] step: [71/12] time: 9.022s, mse: 0.042 \n",
      "Epoch: [0/100] step: [72/12] time: 8.985s, mse: 0.034 \n",
      "Epoch: [0/100] step: [73/12] time: 9.099s, mse: 0.031 \n",
      "Epoch: [0/100] step: [74/12] time: 9.081s, mse: 0.037 \n",
      "Epoch: [0/100] step: [75/12] time: 9.118s, mse: 0.040 \n",
      "Epoch: [0/100] step: [76/12] time: 9.348s, mse: 0.032 \n",
      "Epoch: [0/100] step: [77/12] time: 9.121s, mse: 0.051 \n",
      "Epoch: [0/100] step: [78/12] time: 9.083s, mse: 0.043 \n",
      "Epoch: [0/100] step: [79/12] time: 9.137s, mse: 0.027 \n",
      "Epoch: [0/100] step: [80/12] time: 9.087s, mse: 0.023 \n",
      "Epoch: [0/100] step: [81/12] time: 9.059s, mse: 0.023 \n",
      "Epoch: [0/100] step: [82/12] time: 9.055s, mse: 0.045 \n",
      "Epoch: [0/100] step: [83/12] time: 9.172s, mse: 0.025 \n",
      "Epoch: [0/100] step: [84/12] time: 9.170s, mse: 0.029 \n",
      "Epoch: [0/100] step: [85/12] time: 8.985s, mse: 0.016 \n",
      "Epoch: [0/100] step: [86/12] time: 9.023s, mse: 0.025 \n",
      "Epoch: [0/100] step: [87/12] time: 9.078s, mse: 0.028 \n",
      "Epoch: [0/100] step: [88/12] time: 9.065s, mse: 0.023 \n",
      "Epoch: [0/100] step: [89/12] time: 9.269s, mse: 0.032 \n",
      "Epoch: [0/100] step: [90/12] time: 9.124s, mse: 0.023 \n",
      "Epoch: [0/100] step: [91/12] time: 9.197s, mse: 0.023 \n",
      "Epoch: [0/100] step: [92/12] time: 9.007s, mse: 0.023 \n",
      "Epoch: [0/100] step: [93/12] time: 9.105s, mse: 0.020 \n",
      "Epoch: [0/100] step: [94/12] time: 9.030s, mse: 0.021 \n",
      "Epoch: [0/100] step: [95/12] time: 9.064s, mse: 0.027 \n",
      "Epoch: [0/100] step: [96/12] time: 9.283s, mse: 0.017 \n",
      "Epoch: [0/100] step: [97/12] time: 9.094s, mse: 0.018 \n",
      "Epoch: [0/100] step: [98/12] time: 9.023s, mse: 0.025 \n",
      "Epoch: [0/100] step: [99/12] time: 9.113s, mse: 0.014 \n",
      "Epoch: [0/100] step: [100/12] time: 9.034s, mse: 0.010 \n",
      "Epoch: [0/100] step: [101/12] time: 9.137s, mse: 0.021 \n",
      "Epoch: [0/100] step: [102/12] time: 9.112s, mse: 0.041 \n",
      "Epoch: [0/100] step: [103/12] time: 9.239s, mse: 0.026 \n",
      "Epoch: [0/100] step: [104/12] time: 9.026s, mse: 0.017 \n",
      "Epoch: [0/100] step: [105/12] time: 9.020s, mse: 0.015 \n",
      "Epoch: [0/100] step: [106/12] time: 9.086s, mse: 0.020 \n",
      "Epoch: [0/100] step: [107/12] time: 8.980s, mse: 0.021 \n",
      "Epoch: [0/100] step: [108/12] time: 9.011s, mse: 0.018 \n",
      "Epoch: [0/100] step: [109/12] time: 9.238s, mse: 0.021 \n",
      "Epoch: [0/100] step: [110/12] time: 9.119s, mse: 0.030 \n",
      "Epoch: [0/100] step: [111/12] time: 9.001s, mse: 0.021 \n",
      "Epoch: [0/100] step: [112/12] time: 9.041s, mse: 0.036 \n",
      "Epoch: [0/100] step: [113/12] time: 9.023s, mse: 0.025 \n",
      "Epoch: [0/100] step: [114/12] time: 8.992s, mse: 0.038 \n",
      "Epoch: [0/100] step: [115/12] time: 9.061s, mse: 0.014 \n",
      "Epoch: [0/100] step: [116/12] time: 9.148s, mse: 0.017 \n",
      "Epoch: [0/100] step: [117/12] time: 9.070s, mse: 0.013 \n",
      "Epoch: [0/100] step: [118/12] time: 9.100s, mse: 0.019 \n",
      "Epoch: [0/100] step: [119/12] time: 9.138s, mse: 0.016 \n",
      "Epoch: [0/100] step: [120/12] time: 9.093s, mse: 0.016 \n",
      "Epoch: [0/100] step: [121/12] time: 9.036s, mse: 0.032 \n",
      "Epoch: [0/100] step: [122/12] time: 9.377s, mse: 0.028 \n",
      "Epoch: [0/100] step: [123/12] time: 9.099s, mse: 0.018 \n",
      "Epoch: [0/100] step: [124/12] time: 9.081s, mse: 0.020 \n",
      "Epoch: [0/100] step: [125/12] time: 9.080s, mse: 0.030 \n",
      "Epoch: [0/100] step: [126/12] time: 9.058s, mse: 0.020 \n",
      "Epoch: [0/100] step: [127/12] time: 8.997s, mse: 0.013 \n",
      "Epoch: [0/100] step: [128/12] time: 9.043s, mse: 0.017 \n",
      "Epoch: [0/100] step: [129/12] time: 9.302s, mse: 0.011 \n",
      "Epoch: [0/100] step: [130/12] time: 9.054s, mse: 0.018 \n",
      "Epoch: [0/100] step: [131/12] time: 9.030s, mse: 0.015 \n",
      "Epoch: [0/100] step: [132/12] time: 9.030s, mse: 0.020 \n",
      "Epoch: [0/100] step: [133/12] time: 9.117s, mse: 0.016 \n",
      "Epoch: [0/100] step: [134/12] time: 9.074s, mse: 0.012 \n",
      "Epoch: [0/100] step: [135/12] time: 9.098s, mse: 0.010 \n",
      "Epoch: [0/100] step: [136/12] time: 9.336s, mse: 0.019 \n",
      "Epoch: [0/100] step: [137/12] time: 9.115s, mse: 0.020 \n",
      "Epoch: [0/100] step: [138/12] time: 9.090s, mse: 0.018 \n",
      "Epoch: [0/100] step: [139/12] time: 9.106s, mse: 0.011 \n",
      "Epoch: [0/100] step: [140/12] time: 9.039s, mse: 0.016 \n",
      "Epoch: [0/100] step: [141/12] time: 9.143s, mse: 0.014 \n",
      "Epoch: [0/100] step: [142/12] time: 9.399s, mse: 0.012 \n",
      "Epoch: [0/100] step: [143/12] time: 9.101s, mse: 0.015 \n",
      "Epoch: [0/100] step: [144/12] time: 9.041s, mse: 0.010 \n",
      "Epoch: [0/100] step: [145/12] time: 9.098s, mse: 0.015 \n",
      "Epoch: [0/100] step: [146/12] time: 9.022s, mse: 0.017 \n",
      "Epoch: [0/100] step: [147/12] time: 8.971s, mse: 0.010 \n",
      "Epoch: [0/100] step: [148/12] time: 9.087s, mse: 0.010 \n",
      "Epoch: [0/100] step: [149/12] time: 9.158s, mse: 0.018 \n",
      "Epoch: [0/100] step: [150/12] time: 9.152s, mse: 0.014 \n",
      "Epoch: [0/100] step: [151/12] time: 9.033s, mse: 0.019 \n",
      "Epoch: [0/100] step: [152/12] time: 9.050s, mse: 0.014 \n",
      "Epoch: [0/100] step: [153/12] time: 9.025s, mse: 0.012 \n",
      "Epoch: [0/100] step: [154/12] time: 8.997s, mse: 0.012 \n",
      "Epoch: [0/100] step: [155/12] time: 9.245s, mse: 0.021 \n",
      "Epoch: [0/100] step: [156/12] time: 9.004s, mse: 0.018 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/100] step: [157/12] time: 9.017s, mse: 0.011 \n",
      "Epoch: [0/100] step: [158/12] time: 9.019s, mse: 0.009 \n",
      "Epoch: [0/100] step: [159/12] time: 9.105s, mse: 0.023 \n",
      "Epoch: [0/100] step: [160/12] time: 9.019s, mse: 0.020 \n",
      "Epoch: [0/100] step: [161/12] time: 8.998s, mse: 0.012 \n",
      "Epoch: [0/100] step: [162/12] time: 9.276s, mse: 0.016 \n",
      "Epoch: [0/100] step: [163/12] time: 9.043s, mse: 0.011 \n",
      "Epoch: [0/100] step: [164/12] time: 9.025s, mse: 0.010 \n",
      "Epoch: [0/100] step: [165/12] time: 8.936s, mse: 0.014 \n",
      "Epoch: [0/100] step: [166/12] time: 9.052s, mse: 0.011 \n",
      "Epoch: [0/100] step: [167/12] time: 9.028s, mse: 0.021 \n",
      "Epoch: [0/100] step: [168/12] time: 9.023s, mse: 0.012 \n",
      "Epoch: [0/100] step: [169/12] time: 9.275s, mse: 0.010 \n",
      "Epoch: [0/100] step: [170/12] time: 9.192s, mse: 0.010 \n",
      "Epoch: [0/100] step: [171/12] time: 8.997s, mse: 0.016 \n",
      "Epoch: [0/100] step: [172/12] time: 9.130s, mse: 0.014 \n",
      "Epoch: [0/100] step: [173/12] time: 9.170s, mse: 0.016 \n",
      "Epoch: [0/100] step: [174/12] time: 9.050s, mse: 0.021 \n",
      "Epoch: [0/100] step: [175/12] time: 9.327s, mse: 0.016 \n",
      "Epoch: [0/100] step: [176/12] time: 9.068s, mse: 0.015 \n",
      "Epoch: [0/100] step: [177/12] time: 9.115s, mse: 0.012 \n",
      "Epoch: [0/100] step: [178/12] time: 9.045s, mse: 0.015 \n",
      "Epoch: [0/100] step: [179/12] time: 9.008s, mse: 0.011 \n",
      "Epoch: [0/100] step: [180/12] time: 9.076s, mse: 0.012 \n",
      "Epoch: [0/100] step: [181/12] time: 9.073s, mse: 0.012 \n",
      "Epoch: [0/100] step: [182/12] time: 9.213s, mse: 0.014 \n",
      "Epoch: [0/100] step: [183/12] time: 9.086s, mse: 0.014 \n",
      "Epoch: [0/100] step: [184/12] time: 9.027s, mse: 0.012 \n",
      "Epoch: [0/100] step: [185/12] time: 9.055s, mse: 0.010 \n",
      "Epoch: [0/100] step: [186/12] time: 9.059s, mse: 0.012 \n",
      "Epoch: [0/100] step: [187/12] time: 9.025s, mse: 0.016 \n",
      "Epoch: [0/100] step: [188/12] time: 9.241s, mse: 0.013 \n",
      "Epoch: [0/100] step: [189/12] time: 9.105s, mse: 0.013 \n",
      "Epoch: [0/100] step: [190/12] time: 9.081s, mse: 0.018 \n",
      "Epoch: [0/100] step: [191/12] time: 9.116s, mse: 0.013 \n",
      "Epoch: [0/100] step: [192/12] time: 9.155s, mse: 0.014 \n",
      "Epoch: [0/100] step: [193/12] time: 8.946s, mse: 0.013 \n",
      "Epoch: [0/100] step: [194/12] time: 8.982s, mse: 0.024 \n",
      "Epoch: [0/100] step: [195/12] time: 9.230s, mse: 0.011 \n",
      "Epoch: [0/100] step: [196/12] time: 9.041s, mse: 0.008 \n",
      "Epoch: [0/100] step: [197/12] time: 9.009s, mse: 0.015 \n",
      "Epoch: [0/100] step: [198/12] time: 9.072s, mse: 0.009 \n",
      "Epoch: [0/100] step: [199/12] time: 8.976s, mse: 0.019 \n",
      "Epoch: [0/100] step: [200/12] time: 9.057s, mse: 0.010 \n",
      "Epoch: [0/100] step: [201/12] time: 9.083s, mse: 0.012 \n",
      "Epoch: [0/100] step: [202/12] time: 9.315s, mse: 0.010 \n",
      "Epoch: [0/100] step: [203/12] time: 9.088s, mse: 0.008 \n",
      "Epoch: [0/100] step: [204/12] time: 9.043s, mse: 0.028 \n",
      "Epoch: [0/100] step: [205/12] time: 8.976s, mse: 0.008 \n",
      "Epoch: [0/100] step: [206/12] time: 9.111s, mse: 0.010 \n",
      "Epoch: [0/100] step: [207/12] time: 9.051s, mse: 0.011 \n",
      "Epoch: [0/100] step: [208/12] time: 9.379s, mse: 0.009 \n",
      "Epoch: [0/100] step: [209/12] time: 9.088s, mse: 0.019 \n",
      "Epoch: [0/100] step: [210/12] time: 9.093s, mse: 0.010 \n",
      "Epoch: [0/100] step: [211/12] time: 9.069s, mse: 0.012 \n",
      "Epoch: [0/100] step: [212/12] time: 8.981s, mse: 0.011 \n",
      "Epoch: [0/100] step: [213/12] time: 9.027s, mse: 0.012 \n",
      "Epoch: [0/100] step: [214/12] time: 9.066s, mse: 0.014 \n",
      "Epoch: [0/100] step: [215/12] time: 9.160s, mse: 0.016 \n",
      "Epoch: [0/100] step: [216/12] time: 9.060s, mse: 0.021 \n",
      "Epoch: [0/100] step: [217/12] time: 9.113s, mse: 0.014 \n",
      "Epoch: [0/100] step: [218/12] time: 9.157s, mse: 0.010 \n",
      "Epoch: [0/100] step: [219/12] time: 9.028s, mse: 0.009 \n",
      "Epoch: [0/100] step: [220/12] time: 9.015s, mse: 0.010 \n",
      "Epoch: [0/100] step: [221/12] time: 9.249s, mse: 0.015 \n",
      "Epoch: [0/100] step: [222/12] time: 9.047s, mse: 0.007 \n",
      "Epoch: [0/100] step: [223/12] time: 9.106s, mse: 0.017 \n",
      "Epoch: [0/100] step: [224/12] time: 9.041s, mse: 0.011 \n",
      "Epoch: [0/100] step: [225/12] time: 9.062s, mse: 0.011 \n",
      "Epoch: [0/100] step: [226/12] time: 9.091s, mse: 0.020 \n",
      "Epoch: [0/100] step: [227/12] time: 9.067s, mse: 0.010 \n",
      "Epoch: [0/100] step: [228/12] time: 9.368s, mse: 0.008 \n",
      "Epoch: [0/100] step: [229/12] time: 9.110s, mse: 0.013 \n",
      "Epoch: [0/100] step: [230/12] time: 9.089s, mse: 0.015 \n",
      "Epoch: [0/100] step: [231/12] time: 9.015s, mse: 0.011 \n",
      "Epoch: [0/100] step: [232/12] time: 9.070s, mse: 0.009 \n",
      "Epoch: [0/100] step: [233/12] time: 9.028s, mse: 0.015 \n",
      "Epoch: [0/100] step: [234/12] time: 9.249s, mse: 0.007 \n",
      "Epoch: [0/100] step: [235/12] time: 9.218s, mse: 0.012 \n",
      "Epoch: [0/100] step: [236/12] time: 9.057s, mse: 0.020 \n",
      "Epoch: [0/100] step: [237/12] time: 9.018s, mse: 0.012 \n",
      "Epoch: [0/100] step: [238/12] time: 9.198s, mse: 0.009 \n",
      "Epoch: [0/100] step: [239/12] time: 9.058s, mse: 0.009 \n",
      "Epoch: [0/100] step: [240/12] time: 9.136s, mse: 0.032 \n",
      "Epoch: [0/100] step: [241/12] time: 9.320s, mse: 0.014 \n",
      "Epoch: [0/100] step: [242/12] time: 9.099s, mse: 0.023 \n",
      "Epoch: [0/100] step: [243/12] time: 9.138s, mse: 0.010 \n",
      "Epoch: [0/100] step: [244/12] time: 9.062s, mse: 0.014 \n",
      "Epoch: [0/100] step: [245/12] time: 9.037s, mse: 0.022 \n",
      "Epoch: [0/100] step: [246/12] time: 8.966s, mse: 0.015 \n",
      "Epoch: [0/100] step: [247/12] time: 9.065s, mse: 0.011 \n",
      "Epoch: [0/100] step: [248/12] time: 9.215s, mse: 0.016 \n",
      "Epoch: [0/100] step: [249/12] time: 9.095s, mse: 0.010 \n",
      "Epoch: [0/100] step: [250/12] time: 9.145s, mse: 0.009 \n",
      "Epoch: [0/100] step: [251/12] time: 9.127s, mse: 0.015 \n",
      "Epoch: [0/100] step: [252/12] time: 9.099s, mse: 0.028 \n",
      "Epoch: [0/100] step: [253/12] time: 9.072s, mse: 0.011 \n",
      "Epoch: [0/100] step: [254/12] time: 9.335s, mse: 0.010 \n",
      "Epoch: [0/100] step: [255/12] time: 9.101s, mse: 0.019 \n",
      "Epoch: [0/100] step: [256/12] time: 9.203s, mse: 0.013 \n",
      "Epoch: [0/100] step: [257/12] time: 9.091s, mse: 0.012 \n",
      "Epoch: [0/100] step: [258/12] time: 9.077s, mse: 0.022 \n",
      "Epoch: [0/100] step: [259/12] time: 9.103s, mse: 0.010 \n",
      "Epoch: [0/100] step: [260/12] time: 9.205s, mse: 0.017 \n",
      "Epoch: [0/100] step: [261/12] time: 9.224s, mse: 0.014 \n",
      "Epoch: [0/100] step: [262/12] time: 9.055s, mse: 0.017 \n",
      "Epoch: [0/100] step: [263/12] time: 9.099s, mse: 0.012 \n",
      "Epoch: [0/100] step: [264/12] time: 9.007s, mse: 0.017 \n",
      "Epoch: [0/100] step: [265/12] time: 9.091s, mse: 0.009 \n",
      "Epoch: [0/100] step: [266/12] time: 9.052s, mse: 0.013 \n",
      "Epoch: [0/100] step: [267/12] time: 9.259s, mse: 0.011 \n",
      "Epoch: [0/100] step: [268/12] time: 8.991s, mse: 0.010 \n",
      "Epoch: [0/100] step: [269/12] time: 9.074s, mse: 0.015 \n",
      "Epoch: [0/100] step: [270/12] time: 9.105s, mse: 0.009 \n",
      "Epoch: [0/100] step: [271/12] time: 9.050s, mse: 0.012 \n",
      "Epoch: [0/100] step: [272/12] time: 9.125s, mse: 0.012 \n",
      "Epoch: [0/100] step: [273/12] time: 9.053s, mse: 0.011 \n",
      "Epoch: [0/100] step: [274/12] time: 9.379s, mse: 0.024 \n",
      "Epoch: [0/100] step: [275/12] time: 9.102s, mse: 0.010 \n",
      "Epoch: [0/100] step: [276/12] time: 9.141s, mse: 0.008 \n",
      "Epoch: [0/100] step: [277/12] time: 9.109s, mse: 0.008 \n",
      "Epoch: [0/100] step: [278/12] time: 9.084s, mse: 0.011 \n",
      "Epoch: [0/100] step: [279/12] time: 9.086s, mse: 0.010 \n",
      "Epoch: [0/100] step: [280/12] time: 9.071s, mse: 0.007 \n",
      "Epoch: [0/100] step: [281/12] time: 9.327s, mse: 0.009 \n",
      "Epoch: [0/100] step: [282/12] time: 9.146s, mse: 0.010 \n",
      "Epoch: [0/100] step: [283/12] time: 9.121s, mse: 0.009 \n",
      "Epoch: [0/100] step: [284/12] time: 8.984s, mse: 0.008 \n",
      "Epoch: [0/100] step: [285/12] time: 9.037s, mse: 0.008 \n",
      "Epoch: [0/100] step: [286/12] time: 9.042s, mse: 0.006 \n",
      "Epoch: [0/100] step: [287/12] time: 9.323s, mse: 0.008 \n",
      "Epoch: [0/100] step: [288/12] time: 9.060s, mse: 0.010 \n",
      "Epoch: [0/100] step: [289/12] time: 9.111s, mse: 0.010 \n",
      "Epoch: [0/100] step: [290/12] time: 9.114s, mse: 0.009 \n",
      "Epoch: [0/100] step: [291/12] time: 9.136s, mse: 0.009 \n",
      "Epoch: [0/100] step: [292/12] time: 9.025s, mse: 0.017 \n",
      "Epoch: [0/100] step: [293/12] time: 9.046s, mse: 0.026 \n",
      "Epoch: [0/100] step: [294/12] time: 9.305s, mse: 0.011 \n",
      "Epoch: [0/100] step: [295/12] time: 9.002s, mse: 0.014 \n",
      "Epoch: [0/100] step: [296/12] time: 9.072s, mse: 0.009 \n",
      "Epoch: [0/100] step: [297/12] time: 9.058s, mse: 0.008 \n",
      "Epoch: [0/100] step: [298/12] time: 9.057s, mse: 0.022 \n",
      "Epoch: [0/100] step: [299/12] time: 9.052s, mse: 0.024 \n",
      "Epoch: [0/100] step: [300/12] time: 9.269s, mse: 0.016 \n",
      "Epoch: [0/100] step: [301/12] time: 9.047s, mse: 0.022 \n",
      "Epoch: [0/100] step: [302/12] time: 8.996s, mse: 0.011 \n",
      "Epoch: [0/100] step: [303/12] time: 9.093s, mse: 0.027 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/100] step: [304/12] time: 9.059s, mse: 0.008 \n",
      "Epoch: [0/100] step: [305/12] time: 9.040s, mse: 0.017 \n",
      "Epoch: [0/100] step: [306/12] time: 8.996s, mse: 0.012 \n",
      "Epoch: [0/100] step: [307/12] time: 9.414s, mse: 0.018 \n",
      "Epoch: [0/100] step: [308/12] time: 9.097s, mse: 0.011 \n",
      "Epoch: [0/100] step: [309/12] time: 9.135s, mse: 0.012 \n",
      "Epoch: [0/100] step: [310/12] time: 9.082s, mse: 0.021 \n",
      "Epoch: [0/100] step: [311/12] time: 9.063s, mse: 0.017 \n",
      "Epoch: [0/100] step: [312/12] time: 8.987s, mse: 0.020 \n",
      "Epoch: [0/100] step: [313/12] time: 9.174s, mse: 0.016 \n",
      "Epoch: [0/100] step: [314/12] time: 9.251s, mse: 0.015 \n",
      "Epoch: [0/100] step: [315/12] time: 9.120s, mse: 0.019 \n",
      "Epoch: [0/100] step: [316/12] time: 9.152s, mse: 0.008 \n",
      "Epoch: [0/100] step: [317/12] time: 9.178s, mse: 0.015 \n",
      "Epoch: [0/100] step: [318/12] time: 9.168s, mse: 0.013 \n",
      "Epoch: [0/100] step: [319/12] time: 9.112s, mse: 0.010 \n",
      "Epoch: [0/100] step: [320/12] time: 9.356s, mse: 0.006 \n",
      "Epoch: [0/100] step: [321/12] time: 9.073s, mse: 0.013 \n",
      "Epoch: [0/100] step: [322/12] time: 9.028s, mse: 0.031 \n",
      "Epoch: [0/100] step: [323/12] time: 9.258s, mse: 0.008 \n",
      "Epoch: [0/100] step: [324/12] time: 9.007s, mse: 0.010 \n",
      "Epoch: [0/100] step: [325/12] time: 9.027s, mse: 0.006 \n",
      "Epoch: [0/100] step: [326/12] time: 9.120s, mse: 0.009 \n",
      "Epoch: [0/100] step: [327/12] time: 9.154s, mse: 0.008 \n",
      "Epoch: [0/100] step: [328/12] time: 9.140s, mse: 0.009 \n",
      "Epoch: [0/100] step: [329/12] time: 9.196s, mse: 0.026 \n",
      "Epoch: [0/100] step: [330/12] time: 9.165s, mse: 0.019 \n",
      "Epoch: [0/100] step: [331/12] time: 9.025s, mse: 0.013 \n",
      "Epoch: [0/100] step: [332/12] time: 8.998s, mse: 0.006 \n",
      "Epoch: [0/100] step: [333/12] time: 9.255s, mse: 0.011 \n",
      "Epoch: [0/100] step: [334/12] time: 8.971s, mse: 0.010 \n",
      "Epoch: [0/100] step: [335/12] time: 9.044s, mse: 0.008 \n",
      "Epoch: [0/100] step: [336/12] time: 9.116s, mse: 0.007 \n",
      "Epoch: [0/100] step: [337/12] time: 9.023s, mse: 0.012 \n",
      "Epoch: [0/100] step: [338/12] time: 9.069s, mse: 0.016 \n",
      "Epoch: [0/100] step: [339/12] time: 9.069s, mse: 0.011 \n",
      "Epoch: [0/100] step: [340/12] time: 9.360s, mse: 0.030 \n",
      "Epoch: [0/100] step: [341/12] time: 9.005s, mse: 0.009 \n",
      "Epoch: [0/100] step: [342/12] time: 9.048s, mse: 0.013 \n",
      "Epoch: [0/100] step: [343/12] time: 9.111s, mse: 0.010 \n",
      "Epoch: [0/100] step: [344/12] time: 9.009s, mse: 0.009 \n",
      "Epoch: [0/100] step: [345/12] time: 8.990s, mse: 0.005 \n",
      "Epoch: [0/100] step: [346/12] time: 9.260s, mse: 0.011 \n",
      "Epoch: [0/100] step: [347/12] time: 9.108s, mse: 0.009 \n",
      "Epoch: [0/100] step: [348/12] time: 9.066s, mse: 0.007 \n",
      "Epoch: [0/100] step: [349/12] time: 9.009s, mse: 0.008 \n",
      "Epoch: [0/100] step: [350/12] time: 9.216s, mse: 0.008 \n",
      "Epoch: [0/100] step: [351/12] time: 9.040s, mse: 0.008 \n",
      "Epoch: [0/100] step: [352/12] time: 9.083s, mse: 0.010 \n",
      "Epoch: [0/100] step: [353/12] time: 9.382s, mse: 0.009 \n",
      "Epoch: [0/100] step: [354/12] time: 9.097s, mse: 0.012 \n",
      "Epoch: [0/100] step: [355/12] time: 9.001s, mse: 0.008 \n",
      "Epoch: [0/100] step: [356/12] time: 9.118s, mse: 0.006 \n",
      "Epoch: [0/100] step: [357/12] time: 9.163s, mse: 0.007 \n",
      "Epoch: [0/100] step: [358/12] time: 9.089s, mse: 0.007 \n",
      "Epoch: [0/100] step: [359/12] time: 9.052s, mse: 0.009 \n",
      "Epoch: [0/100] step: [360/12] time: 9.286s, mse: 0.013 \n",
      "Epoch: [0/100] step: [361/12] time: 9.090s, mse: 0.008 \n",
      "Epoch: [0/100] step: [362/12] time: 9.227s, mse: 0.010 \n",
      "Epoch: [0/100] step: [363/12] time: 9.077s, mse: 0.015 \n",
      "Epoch: [0/100] step: [364/12] time: 9.145s, mse: 0.008 \n",
      "Epoch: [0/100] step: [365/12] time: 9.105s, mse: 0.007 \n",
      "Epoch: [0/100] step: [366/12] time: 9.260s, mse: 0.014 \n",
      "Epoch: [0/100] step: [367/12] time: 9.040s, mse: 0.020 \n",
      "Epoch: [0/100] step: [368/12] time: 8.995s, mse: 0.029 \n",
      "Epoch: [0/100] step: [369/12] time: 9.032s, mse: 0.012 \n",
      "Epoch: [0/100] step: [370/12] time: 9.000s, mse: 0.010 \n",
      "Epoch: [0/100] step: [371/12] time: 9.065s, mse: 0.007 \n",
      "Epoch: [0/100] step: [372/12] time: 9.049s, mse: 0.006 \n",
      "Epoch: [0/100] step: [373/12] time: 9.224s, mse: 0.008 \n",
      "Epoch: [0/100] step: [374/12] time: 9.133s, mse: 0.013 \n",
      "Epoch: [0/100] step: [375/12] time: 9.045s, mse: 0.007 \n",
      "Epoch: [0/100] step: [376/12] time: 9.103s, mse: 0.009 \n",
      "Epoch: [0/100] step: [377/12] time: 9.099s, mse: 0.007 \n",
      "Epoch: [0/100] step: [378/12] time: 9.000s, mse: 0.006 \n",
      "Epoch: [0/100] step: [379/12] time: 9.244s, mse: 0.006 \n",
      "Epoch: [0/100] step: [380/12] time: 9.094s, mse: 0.014 \n",
      "Epoch: [0/100] step: [381/12] time: 9.072s, mse: 0.009 \n",
      "Epoch: [0/100] step: [382/12] time: 9.104s, mse: 0.019 \n",
      "Epoch: [0/100] step: [383/12] time: 9.061s, mse: 0.012 \n",
      "Epoch: [0/100] step: [384/12] time: 9.077s, mse: 0.010 \n",
      "Epoch: [0/100] step: [385/12] time: 9.042s, mse: 0.009 \n",
      "Epoch: [0/100] step: [386/12] time: 9.299s, mse: 0.008 \n",
      "Epoch: [0/100] step: [387/12] time: 9.124s, mse: 0.008 \n",
      "Epoch: [0/100] step: [388/12] time: 9.050s, mse: 0.007 \n",
      "Epoch: [0/100] step: [389/12] time: 9.166s, mse: 0.012 \n",
      "Epoch: [0/100] step: [390/12] time: 9.010s, mse: 0.008 \n",
      "Epoch: [0/100] step: [391/12] time: 9.086s, mse: 0.006 \n",
      "Epoch: [0/100] step: [392/12] time: 9.054s, mse: 0.006 \n",
      "Epoch: [0/100] step: [393/12] time: 9.206s, mse: 0.010 \n",
      "Epoch: [0/100] step: [394/12] time: 9.046s, mse: 0.006 \n",
      "Epoch: [0/100] step: [395/12] time: 8.980s, mse: 0.009 \n",
      "Epoch: [0/100] step: [396/12] time: 9.164s, mse: 0.007 \n",
      "Epoch: [0/100] step: [397/12] time: 9.048s, mse: 0.006 \n",
      "Epoch: [0/100] step: [398/12] time: 9.109s, mse: 0.018 \n",
      "Epoch: [0/100] step: [399/12] time: 9.272s, mse: 0.007 \n",
      "Epoch: [1/100] step: [0/12] time: 9.075s, mse: 0.010 \n",
      "Epoch: [1/100] step: [1/12] time: 9.058s, mse: 0.010 \n",
      "Epoch: [1/100] step: [2/12] time: 9.227s, mse: 0.009 \n",
      "Epoch: [1/100] step: [3/12] time: 9.062s, mse: 0.007 \n",
      "Epoch: [1/100] step: [4/12] time: 9.104s, mse: 0.016 \n",
      "Epoch: [1/100] step: [5/12] time: 9.207s, mse: 0.006 \n",
      "Epoch: [1/100] step: [6/12] time: 9.242s, mse: 0.007 \n",
      "Epoch: [1/100] step: [7/12] time: 9.095s, mse: 0.005 \n",
      "Epoch: [1/100] step: [8/12] time: 9.077s, mse: 0.008 \n",
      "Epoch: [1/100] step: [9/12] time: 9.093s, mse: 0.010 \n",
      "Epoch: [1/100] step: [10/12] time: 9.049s, mse: 0.007 \n",
      "Epoch: [1/100] step: [11/12] time: 9.048s, mse: 0.012 \n",
      "Epoch: [1/100] step: [12/12] time: 9.236s, mse: 0.010 \n",
      "Epoch: [1/100] step: [13/12] time: 9.101s, mse: 0.010 \n",
      "Epoch: [1/100] step: [14/12] time: 9.031s, mse: 0.015 \n",
      "Epoch: [1/100] step: [15/12] time: 9.057s, mse: 0.014 \n",
      "Epoch: [1/100] step: [16/12] time: 9.175s, mse: 0.007 \n",
      "Epoch: [1/100] step: [17/12] time: 9.073s, mse: 0.005 \n",
      "Epoch: [1/100] step: [18/12] time: 9.030s, mse: 0.008 \n",
      "Epoch: [1/100] step: [19/12] time: 9.293s, mse: 0.011 \n",
      "Epoch: [1/100] step: [20/12] time: 9.036s, mse: 0.009 \n",
      "Epoch: [1/100] step: [21/12] time: 9.072s, mse: 0.009 \n",
      "Epoch: [1/100] step: [22/12] time: 9.149s, mse: 0.009 \n",
      "Epoch: [1/100] step: [23/12] time: 9.060s, mse: 0.010 \n",
      "Epoch: [1/100] step: [24/12] time: 9.093s, mse: 0.008 \n",
      "Epoch: [1/100] step: [25/12] time: 9.283s, mse: 0.011 \n",
      "Epoch: [1/100] step: [26/12] time: 9.037s, mse: 0.006 \n",
      "Epoch: [1/100] step: [27/12] time: 9.097s, mse: 0.008 \n",
      "Epoch: [1/100] step: [28/12] time: 9.126s, mse: 0.010 \n",
      "Epoch: [1/100] step: [29/12] time: 9.155s, mse: 0.008 \n",
      "Epoch: [1/100] step: [30/12] time: 9.048s, mse: 0.013 \n",
      "Epoch: [1/100] step: [31/12] time: 9.087s, mse: 0.006 \n",
      "Epoch: [1/100] step: [32/12] time: 9.247s, mse: 0.005 \n",
      "Epoch: [1/100] step: [33/12] time: 9.027s, mse: 0.008 \n",
      "Epoch: [1/100] step: [34/12] time: 9.002s, mse: 0.009 \n",
      "Epoch: [1/100] step: [35/12] time: 9.177s, mse: 0.006 \n",
      "Epoch: [1/100] step: [36/12] time: 9.086s, mse: 0.007 \n",
      "Epoch: [1/100] step: [37/12] time: 9.027s, mse: 0.007 \n",
      "Epoch: [1/100] step: [38/12] time: 9.098s, mse: 0.010 \n",
      "Epoch: [1/100] step: [39/12] time: 9.230s, mse: 0.007 \n",
      "Epoch: [1/100] step: [40/12] time: 9.014s, mse: 0.006 \n",
      "Epoch: [1/100] step: [41/12] time: 9.120s, mse: 0.009 \n",
      "Epoch: [1/100] step: [42/12] time: 9.219s, mse: 0.007 \n",
      "Epoch: [1/100] step: [43/12] time: 8.907s, mse: 0.010 \n",
      "Epoch: [1/100] step: [44/12] time: 9.082s, mse: 0.006 \n",
      "Epoch: [1/100] step: [45/12] time: 9.330s, mse: 0.006 \n",
      "Epoch: [1/100] step: [46/12] time: 9.063s, mse: 0.009 \n",
      "Epoch: [1/100] step: [47/12] time: 9.079s, mse: 0.011 \n",
      "Epoch: [1/100] step: [48/12] time: 9.068s, mse: 0.006 \n",
      "Epoch: [1/100] step: [49/12] time: 9.107s, mse: 0.006 \n",
      "Epoch: [1/100] step: [50/12] time: 9.063s, mse: 0.013 \n",
      "Epoch: [1/100] step: [51/12] time: 9.034s, mse: 0.007 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100] step: [52/12] time: 9.275s, mse: 0.011 \n",
      "Epoch: [1/100] step: [53/12] time: 8.947s, mse: 0.006 \n"
     ]
    }
   ],
   "source": [
    "## initialize learning (G)\n",
    "n_step_epoch = round(n_epoch_init // batch_size)\n",
    "for epoch in range(n_epoch_init):\n",
    "    for step, lr_patchs in enumerate(train_ds):\n",
    "        if lr_patchs.shape[0] != batch_size: # if the remaining data in this epoch < batch_size\n",
    "            break\n",
    "        step_time = time.time()\n",
    "        with tf.GradientTape() as tape:\n",
    "            fake_hr_patchs = G(lr_patchs)\n",
    "            fake_lr_patches = downscale_hr_patches(fake_hr_patchs)\n",
    "\n",
    "            mse_loss = tl.cost.mean_squared_error(fake_lr_patches, lr_patchs, is_mean=True)\n",
    "\n",
    "            grad = tape.gradient(mse_loss, G.trainable_weights)\n",
    "            g_optimizer_init.apply_gradients(zip(grad, G.trainable_weights))\n",
    "\n",
    "        with open(\"logs/init_generator.log\", \"a\") as log_file:\n",
    "            log_file.write(\"Epoch: [{}/{}] step: [{}/{}] time: {:.3f}s, mse: {:.3f} \".format(\n",
    "            epoch, n_epoch_init, step, n_step_epoch, time.time() - step_time, mse_loss))\n",
    "            log_file.close()\n",
    "    \n",
    "        print(\"Epoch: [{}/{}] step: [{}/{}] time: {:.3f}s, mse: {:.3f} \".format(\n",
    "            epoch, n_epoch_init, step, n_step_epoch, time.time() - step_time, mse_loss))\n",
    "    if (epoch != 0) and (epoch % 10 == 0):\n",
    "        tl.vis.save_images(fake_hr_patchs.numpy(), [2, 4], os.path.join(save_dir, 'train_g_init_{}.png'.format(epoch)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.save_weights(os.path.join(checkpoint_dir, 'g-initial.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G.load_weights(os.path.join(checkpoint_dir, 'g-initial.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## adversarial learning (G, D)\n",
    "n_step_epoch = round(n_epoch // batch_size)\n",
    "for epoch in range(n_epoch):\n",
    "    for step, lr_patchs in enumerate(train_ds):\n",
    "        if lr_patchs.shape[0] != batch_size: # if the remaining data in this epoch < batch_size\n",
    "            break\n",
    "        step_time = time.time()\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            fake_patchs = G(lr_patchs)\n",
    "            fake_lr_patches = downscale_hr_patches(fake_patchs)\n",
    "            \n",
    "            logits_fake = D(fake_patchs)\n",
    "            logits_real = D(fake_lr_patches)\n",
    "            \n",
    "            feature_fake = VGG((fake_patchs+1)/2.) # the pre-trained VGG uses the input range of [0, 1]\n",
    "            feature_real = VGG((fake_lr_patches+1)/2.)\n",
    "            \n",
    "            d_loss1 = tl.cost.sigmoid_cross_entropy(logits_real, tf.ones_like(logits_real))\n",
    "            d_loss2 = tl.cost.sigmoid_cross_entropy(logits_fake, tf.zeros_like(logits_fake))\n",
    "            \n",
    "            d_loss = d_loss1 + d_loss2\n",
    "            g_gan_loss = 1e-3 * tl.cost.sigmoid_cross_entropy(logits_fake, tf.ones_like(logits_fake))\n",
    "\n",
    "            # mse_loss = tl.cost.mean_squared_error(fake_patchs, hr_patchs, is_mean=True)\n",
    "            \n",
    "            \n",
    "\n",
    "            mse_loss = tl.cost.mean_squared_error(fake_lr_patches, lr_patchs, is_mean=True)\n",
    "\n",
    "            vgg_loss = 2e-6 * tl.cost.mean_squared_error(feature_fake, feature_real, is_mean=True)\n",
    "            \n",
    "            g_loss = mse_loss + vgg_loss + g_gan_loss\n",
    "            \n",
    "            grad = tape.gradient(g_loss, G.trainable_weights)\n",
    "            g_optimizer.apply_gradients(zip(grad, G.trainable_weights))\n",
    "            grad = tape.gradient(d_loss, D.trainable_weights)\n",
    "            d_optimizer.apply_gradients(zip(grad, D.trainable_weights))\n",
    "        \n",
    "        with open(\"logs/gan.log\", \"a\") as log_file:\n",
    "            log_file.write(\"Epoch: [{}/{}] step: [{}/{}] time: {:.3f}s, g_loss(mse:{:.3f}, vgg:{:.3f}, adv:{:.3f}) d_loss: {:.3f}\".format(\n",
    "            epoch, n_epoch_init, step, n_step_epoch, time.time() - step_time, mse_loss, vgg_loss, g_gan_loss, d_loss))\n",
    "            log_file.close()\n",
    "            \n",
    "        print(\"Epoch: [{}/{}] step: [{}/{}] time: {:.3f}s, g_loss(mse:{:.3f}, vgg:{:.3f}, adv:{:.3f}) d_loss: {:.3f}\".format(\n",
    "            epoch, n_epoch_init, step, n_step_epoch, time.time() - step_time, mse_loss, vgg_loss, g_gan_loss, d_loss))\n",
    "\n",
    "    # update the learning rate\n",
    "    if epoch != 0 and (epoch % decay_every == 0):\n",
    "        new_lr_decay = lr_decay**(epoch // decay_every)\n",
    "        lr_v.assign(lr_init * new_lr_decay)\n",
    "        log = \" ** new learning rate: %f (for GAN)\" % (lr_init * new_lr_decay)\n",
    "        \n",
    "        with open(\"logs/learning_rate.log\", \"a\") as log_file:\n",
    "            log_file.write(log)\n",
    "            log_file.close()\n",
    "            \n",
    "        print(log)\n",
    "\n",
    "    if (epoch != 0) and (epoch % 10 == 0):\n",
    "        tl.vis.save_images(fake_patchs.numpy(), [2, 4], os.path.join(save_dir, 'train_g_{}.png'.format(epoch)))\n",
    "        G.save_weights(os.path.join(checkpoint_dir, 'g-{epoch}.h5'.format(epoch=epoch)))\n",
    "        D.save_weights(os.path.join(checkpoint_dir, 'd-{epoch}.h5'.format(epoch=epoch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-1.8",
   "language": "python",
   "name": "tensorflow-1.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
