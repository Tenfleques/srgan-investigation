{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import scipy, multiprocessing\n",
    "import tensorflow as tf\n",
    "import tensorlayer as tl\n",
    "from model import get_G, get_D\n",
    "from config import config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###====================== HYPER-PARAMETERS ===========================###\n",
    "\n",
    "batch_size = config.TRAIN.batch_size  # use 8 if your GPU memory is small, and change [4, 4] in tl.vis.save_images to [2, 4]\n",
    "lr_init = config.TRAIN.lr_init\n",
    "beta1 = config.TRAIN.beta1\n",
    "## initialize G\n",
    "n_epoch_init = config.TRAIN.n_epoch_init\n",
    "\n",
    "n_epoch = config.TRAIN.n_epoch\n",
    "lr_decay = config.TRAIN.lr_decay\n",
    "decay_every = config.TRAIN.decay_every\n",
    "shuffle_buffer_size = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TL] [!] samples exists ...\n",
      "[TL] [!] checkpoint exists ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create folders to save result images and trained models\n",
    "save_dir = \"samples\"\n",
    "tl.files.exists_or_mkdir(save_dir)\n",
    "checkpoint_dir = \"checkpoint\"\n",
    "tl.files.exists_or_mkdir(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data():\n",
    "    # load dataset\n",
    "#     train_hr_img_list = sorted(tl.files.load_file_list(path=config.TRAIN.hr_img_path, regx='.*.png', printable=False))[:3000]\n",
    "    train_lr_img_list = sorted(tl.files.load_file_list(path='DIV2K/DIV2K_train_LR_wild/', regx='.*.png', printable=False))\n",
    "\n",
    "    ## If your machine have enough memory, please pre-load the entire train set.\n",
    "#     train_hr_imgs = tl.vis.read_images(train_hr_img_list, path=config.TRAIN.hr_img_path, n_threads=32)\n",
    "    train_lr_imgs = tl.vis.read_images(train_lr_img_list, path='DIV2K/DIV2K_train_LR_wild/', n_threads=32)\n",
    "        \n",
    "    # dataset API and augmentation\n",
    "    def generator_train():\n",
    "        for img in train_lr_imgs:\n",
    "            yield img\n",
    "            \n",
    "    def _map_fn_train(img):\n",
    "#         hr_patch = tf.image.random_crop(img, [384, 384, 3])\n",
    "#         hr_patch = hr_patch / (255. / 2.)\n",
    "#         hr_patch = hr_patch - 1.\n",
    "#         hr_patch = tf.image.random_flip_left_right(hr_patch)\n",
    "        \n",
    "        lr_patch = tf.image.random_crop(img, [96, 96, 3])\n",
    "        lr_patch = lr_patch / (255. / 2.)\n",
    "        lr_patch = lr_patch - 1.\n",
    "        lr_patch = tf.image.random_flip_left_right(lr_patch)\n",
    "    \n",
    "        return lr_patch\n",
    "    \n",
    "    train_ds = tf.data.Dataset.from_generator(generator_train, output_types=(tf.float32))\n",
    "    train_ds = train_ds.map(_map_fn_train, num_parallel_calls=multiprocessing.cpu_count())\n",
    "        # train_ds = train_ds.repeat(n_epoch_init + n_epoch)\n",
    "    train_ds = train_ds.shuffle(shuffle_buffer_size)\n",
    "    train_ds = train_ds.prefetch(buffer_size=2)\n",
    "    train_ds = train_ds.batch(batch_size)\n",
    "        # value = train_ds.make_one_shot_iterator().get_next()\n",
    "    return train_ds\n",
    "\n",
    "def downscale_hr_patches(hr_patch):\n",
    "    return tf.image.resize(hr_patch, size=[96, 96])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TL] Input  _inputlayer_1: (8, 96, 96, 3)\n",
      "[TL] Conv2d conv2d_1: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: relu\n",
      "[TL] Conv2d conv2d_2: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_1: decay: 0.900000 epsilon: 0.000010 act: relu is_train: False\n",
      "[TL] Conv2d conv2d_3: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_2: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: False\n",
      "[TL] Elementwise elementwise_1: fn: add act: No Activation\n",
      "[TL] Conv2d conv2d_4: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_3: decay: 0.900000 epsilon: 0.000010 act: relu is_train: False\n",
      "[TL] Conv2d conv2d_5: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_4: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: False\n",
      "[TL] Elementwise elementwise_2: fn: add act: No Activation\n",
      "[TL] Conv2d conv2d_6: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_5: decay: 0.900000 epsilon: 0.000010 act: relu is_train: False\n",
      "[TL] Conv2d conv2d_7: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_6: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: False\n",
      "[TL] Elementwise elementwise_3: fn: add act: No Activation\n",
      "[TL] Conv2d conv2d_8: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_7: decay: 0.900000 epsilon: 0.000010 act: relu is_train: False\n",
      "[TL] Conv2d conv2d_9: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_8: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: False\n",
      "[TL] Elementwise elementwise_4: fn: add act: No Activation\n",
      "[TL] Conv2d conv2d_10: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_9: decay: 0.900000 epsilon: 0.000010 act: relu is_train: False\n",
      "[TL] Conv2d conv2d_11: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_10: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: False\n",
      "[TL] Elementwise elementwise_5: fn: add act: No Activation\n",
      "[TL] Conv2d conv2d_12: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_11: decay: 0.900000 epsilon: 0.000010 act: relu is_train: False\n",
      "[TL] Conv2d conv2d_13: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_12: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: False\n",
      "[TL] Elementwise elementwise_6: fn: add act: No Activation\n",
      "[TL] Conv2d conv2d_14: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_13: decay: 0.900000 epsilon: 0.000010 act: relu is_train: False\n",
      "[TL] Conv2d conv2d_15: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_14: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: False\n",
      "[TL] Elementwise elementwise_7: fn: add act: No Activation\n",
      "[TL] Conv2d conv2d_16: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_15: decay: 0.900000 epsilon: 0.000010 act: relu is_train: False\n",
      "[TL] Conv2d conv2d_17: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_16: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: False\n",
      "[TL] Elementwise elementwise_8: fn: add act: No Activation\n",
      "[TL] Conv2d conv2d_18: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_17: decay: 0.900000 epsilon: 0.000010 act: relu is_train: False\n",
      "[TL] Conv2d conv2d_19: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_18: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: False\n",
      "[TL] Elementwise elementwise_9: fn: add act: No Activation\n",
      "[TL] Conv2d conv2d_20: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_19: decay: 0.900000 epsilon: 0.000010 act: relu is_train: False\n",
      "[TL] Conv2d conv2d_21: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_20: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: False\n",
      "[TL] Elementwise elementwise_10: fn: add act: No Activation\n",
      "[TL] Conv2d conv2d_22: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_21: decay: 0.900000 epsilon: 0.000010 act: relu is_train: False\n",
      "[TL] Conv2d conv2d_23: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_22: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: False\n",
      "[TL] Elementwise elementwise_11: fn: add act: No Activation\n",
      "[TL] Conv2d conv2d_24: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_23: decay: 0.900000 epsilon: 0.000010 act: relu is_train: False\n",
      "[TL] Conv2d conv2d_25: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_24: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: False\n",
      "[TL] Elementwise elementwise_12: fn: add act: No Activation\n",
      "[TL] Conv2d conv2d_26: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_25: decay: 0.900000 epsilon: 0.000010 act: relu is_train: False\n",
      "[TL] Conv2d conv2d_27: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_26: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: False\n",
      "[TL] Elementwise elementwise_13: fn: add act: No Activation\n",
      "[TL] Conv2d conv2d_28: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_27: decay: 0.900000 epsilon: 0.000010 act: relu is_train: False\n",
      "[TL] Conv2d conv2d_29: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_28: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: False\n",
      "[TL] Elementwise elementwise_14: fn: add act: No Activation\n",
      "[TL] Conv2d conv2d_30: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_29: decay: 0.900000 epsilon: 0.000010 act: relu is_train: False\n",
      "[TL] Conv2d conv2d_31: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_30: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: False\n",
      "[TL] Elementwise elementwise_15: fn: add act: No Activation\n",
      "[TL] Conv2d conv2d_32: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_31: decay: 0.900000 epsilon: 0.000010 act: relu is_train: False\n",
      "[TL] Conv2d conv2d_33: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_32: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: False\n",
      "[TL] Elementwise elementwise_16: fn: add act: No Activation\n",
      "[TL] Conv2d conv2d_34: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_33: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: False\n",
      "[TL] Elementwise elementwise_17: fn: add act: No Activation\n",
      "[TL] Conv2d conv2d_35: n_filter: 256 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] SubpixelConv2d  subpixelconv2d_1: scale: 2 act: relu\n",
      "[TL] Conv2d conv2d_36: n_filter: 256 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] SubpixelConv2d  subpixelconv2d_2: scale: 2 act: relu\n",
      "[TL] Conv2d conv2d_37: n_filter: 3 filter_size: (1, 1) strides: (1, 1) pad: SAME act: tanh\n",
      "[TL] Input  _inputlayer_2: (8, 384, 384, 3)\n",
      "[TL] Conv2d conv2d_38: n_filter: 64 filter_size: (4, 4) strides: (2, 2) pad: SAME act: <lambda>\n",
      "[TL] Conv2d conv2d_39: n_filter: 128 filter_size: (4, 4) strides: (2, 2) pad: SAME act: No Activation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TL] BatchNorm batchnorm2d_34: decay: 0.900000 epsilon: 0.000010 act: <lambda> is_train: False\n",
      "[TL] Conv2d conv2d_40: n_filter: 256 filter_size: (4, 4) strides: (2, 2) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_35: decay: 0.900000 epsilon: 0.000010 act: <lambda> is_train: False\n",
      "[TL] Conv2d conv2d_41: n_filter: 512 filter_size: (4, 4) strides: (2, 2) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_36: decay: 0.900000 epsilon: 0.000010 act: <lambda> is_train: False\n",
      "[TL] Conv2d conv2d_42: n_filter: 1024 filter_size: (4, 4) strides: (2, 2) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_37: decay: 0.900000 epsilon: 0.000010 act: <lambda> is_train: False\n",
      "[TL] Conv2d conv2d_43: n_filter: 2048 filter_size: (4, 4) strides: (2, 2) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_38: decay: 0.900000 epsilon: 0.000010 act: <lambda> is_train: False\n",
      "[TL] Conv2d conv2d_44: n_filter: 1024 filter_size: (1, 1) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_39: decay: 0.900000 epsilon: 0.000010 act: <lambda> is_train: False\n",
      "[TL] Conv2d conv2d_45: n_filter: 512 filter_size: (1, 1) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_40: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: False\n",
      "[TL] Conv2d conv2d_46: n_filter: 128 filter_size: (1, 1) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_41: decay: 0.900000 epsilon: 0.000010 act: <lambda> is_train: False\n",
      "[TL] Conv2d conv2d_47: n_filter: 128 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_42: decay: 0.900000 epsilon: 0.000010 act: <lambda> is_train: False\n",
      "[TL] Conv2d conv2d_48: n_filter: 512 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
      "[TL] BatchNorm batchnorm2d_43: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: False\n",
      "[TL] Elementwise elementwise_18: fn: add act: <lambda>\n",
      "[TL] Flatten flatten_1:\n",
      "[TL] Dense  dense_1: 1 No Activation\n",
      "[TL] Input  _inputlayer_3: [None, 224, 224, 3]\n",
      "[TL] Lambda  scale: func: <function VGG_static.<locals>.<lambda> at 0x7f8260015f28>, len_weights: 0\n",
      "[TL] Conv2d conv1_1: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: relu\n",
      "[TL] Conv2d conv1_2: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: relu\n",
      "[TL] MaxPool2d pool1: filter_size: (2, 2) strides: (2, 2) padding: SAME\n",
      "[TL] Conv2d conv2_1: n_filter: 128 filter_size: (3, 3) strides: (1, 1) pad: SAME act: relu\n",
      "[TL] Conv2d conv2_2: n_filter: 128 filter_size: (3, 3) strides: (1, 1) pad: SAME act: relu\n",
      "[TL] MaxPool2d pool2: filter_size: (2, 2) strides: (2, 2) padding: SAME\n",
      "[TL] Conv2d conv3_1: n_filter: 256 filter_size: (3, 3) strides: (1, 1) pad: SAME act: relu\n",
      "[TL] Conv2d conv3_2: n_filter: 256 filter_size: (3, 3) strides: (1, 1) pad: SAME act: relu\n",
      "[TL] Conv2d conv3_3: n_filter: 256 filter_size: (3, 3) strides: (1, 1) pad: SAME act: relu\n",
      "[TL] Conv2d conv3_4: n_filter: 256 filter_size: (3, 3) strides: (1, 1) pad: SAME act: relu\n",
      "[TL] MaxPool2d pool3: filter_size: (2, 2) strides: (2, 2) padding: SAME\n",
      "[TL] Conv2d conv4_1: n_filter: 512 filter_size: (3, 3) strides: (1, 1) pad: SAME act: relu\n",
      "[TL] Conv2d conv4_2: n_filter: 512 filter_size: (3, 3) strides: (1, 1) pad: SAME act: relu\n",
      "[TL] Conv2d conv4_3: n_filter: 512 filter_size: (3, 3) strides: (1, 1) pad: SAME act: relu\n",
      "[TL] Conv2d conv4_4: n_filter: 512 filter_size: (3, 3) strides: (1, 1) pad: SAME act: relu\n",
      "[TL] MaxPool2d pool4: filter_size: (2, 2) strides: (2, 2) padding: SAME\n",
      "[TL] LayerList layerlist_1 including layers [conv1_1, conv1_2, pool1, conv2_1, conv2_2, pool2, conv3_1, conv3_2, conv3_3, conv3_4, pool3, conv4_1, conv4_2, conv4_3, conv4_4, pool4]\n",
      "[TL] Restore pre-trained weights\n",
      "[TL]   Loading (3, 3, 3, 64) in conv1_1\n",
      "[TL]   Loading (64,) in conv1_1\n",
      "[TL]   Loading (3, 3, 64, 64) in conv1_2\n",
      "[TL]   Loading (64,) in conv1_2\n",
      "[TL]   Loading (3, 3, 64, 128) in conv2_1\n",
      "[TL]   Loading (128,) in conv2_1\n",
      "[TL]   Loading (3, 3, 128, 128) in conv2_2\n",
      "[TL]   Loading (128,) in conv2_2\n",
      "[TL]   Loading (3, 3, 128, 256) in conv3_1\n",
      "[TL]   Loading (256,) in conv3_1\n",
      "[TL]   Loading (3, 3, 256, 256) in conv3_2\n",
      "[TL]   Loading (256,) in conv3_2\n",
      "[TL]   Loading (3, 3, 256, 256) in conv3_3\n",
      "[TL]   Loading (256,) in conv3_3\n",
      "[TL]   Loading (3, 3, 256, 256) in conv3_4\n",
      "[TL]   Loading (256,) in conv3_4\n",
      "[TL]   Loading (3, 3, 256, 512) in conv4_1\n",
      "[TL]   Loading (512,) in conv4_1\n",
      "[TL]   Loading (3, 3, 512, 512) in conv4_2\n",
      "[TL]   Loading (512,) in conv4_2\n",
      "[TL]   Loading (3, 3, 512, 512) in conv4_3\n",
      "[TL]   Loading (512,) in conv4_3\n",
      "[TL]   Loading (3, 3, 512, 512) in conv4_4\n",
      "[TL]   Loading (512,) in conv4_4\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'DIV2/DIV2K_train_LR_wild/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-60c432fed30a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mVGG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtrain_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_train_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-1cfb6b2c2fd1>\u001b[0m in \u001b[0;36mget_train_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# load dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#     train_hr_img_list = sorted(tl.files.load_file_list(path=config.TRAIN.hr_img_path, regx='.*.png', printable=False))[:3000]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain_lr_img_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_file_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'DIV2/DIV2K_train_LR_wild/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'.*.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprintable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m## If your machine have enough memory, please pre-load the entire train set.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorlayer/files/utils.py\u001b[0m in \u001b[0;36mload_file_list\u001b[0;34m(path, regx, printable, keep_prefix)\u001b[0m\n\u001b[1;32m   2341\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2342\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2343\u001b[0;31m     \u001b[0mfile_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2344\u001b[0m     \u001b[0mreturn_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2345\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'DIV2/DIV2K_train_LR_wild/'"
     ]
    }
   ],
   "source": [
    "G = get_G((batch_size, 96, 96, 3))\n",
    "D = get_D((batch_size, 384, 384, 3))\n",
    "VGG = tl.models.vgg19(pretrained=True, end_with='pool4', mode='static')\n",
    "\n",
    "lr_v = tf.Variable(lr_init)\n",
    "g_optimizer_init = tf.optimizers.Adam(lr_v, beta_1=beta1)\n",
    "g_optimizer = tf.optimizers.Adam(lr_v, beta_1=beta1)\n",
    "d_optimizer = tf.optimizers.Adam(lr_v, beta_1=beta1)\n",
    "\n",
    "G.train()\n",
    "D.train()\n",
    "VGG.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TL] read 32 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 64 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 96 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 128 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 160 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 192 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 224 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 256 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 288 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 320 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 352 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 384 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 416 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 448 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 480 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 512 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 544 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 576 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 608 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 640 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 672 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 704 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 736 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 768 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 800 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 832 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 864 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 896 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 928 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 960 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 992 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 1024 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 1056 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 1088 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 1120 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 1152 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 1184 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 1216 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 1248 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 1280 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 1312 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 1344 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 1376 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 1408 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 1440 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 1472 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 1504 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 1536 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 1568 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 1600 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 1632 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 1664 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 1696 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 1728 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 1760 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 1792 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 1824 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 1856 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 1888 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 1920 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 1952 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 1984 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 2016 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 2048 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 2080 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 2112 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 2144 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 2176 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 2208 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 2240 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 2272 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 2304 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 2336 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 2368 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 2400 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 2432 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 2464 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 2496 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 2528 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 2560 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 2592 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 2624 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 2656 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 2688 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 2720 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 2752 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 2784 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 2816 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 2848 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 2880 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 2976 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 3008 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 3040 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 3072 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 3104 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 3136 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 3168 from DIV2K/DIV2K_train_LR_wild/\n",
      "[TL] read 3200 from DIV2K/DIV2K_train_LR_wild/\n"
     ]
    }
   ],
   "source": [
    "train_ds = get_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialize learning (G)\n",
    "n_step_epoch = round(n_epoch_init // batch_size)\n",
    "for epoch in range(n_epoch_init):\n",
    "    for step, lr_patchs in enumerate(train_ds):\n",
    "        if lr_patchs.shape[0] != batch_size: # if the remaining data in this epoch < batch_size\n",
    "            break\n",
    "        step_time = time.time()\n",
    "        with tf.GradientTape() as tape:\n",
    "            fake_hr_patchs = G(lr_patchs)\n",
    "            fake_lr_patches = downscale_hr_patches(fake_hr_patchs)\n",
    "\n",
    "            mse_loss = tl.cost.mean_squared_error(fake_lr_patches, lr_patchs, is_mean=True)\n",
    "\n",
    "            grad = tape.gradient(mse_loss, G.trainable_weights)\n",
    "            g_optimizer_init.apply_gradients(zip(grad, G.trainable_weights))\n",
    "\n",
    "        with open(\"logs/init_generator.log\", \"a\") as log_file:\n",
    "            log_file.write(\"Epoch: [{}/{}] step: [{}/{}] time: {:.3f}s, mse: {:.3f} \".format(\n",
    "            epoch, n_epoch_init, step, n_step_epoch, time.time() - step_time, mse_loss))\n",
    "            log_file.close()\n",
    "    \n",
    "        print(\"Epoch: [{}/{}] step: [{}/{}] time: {:.3f}s, mse: {:.3f} \".format(\n",
    "            epoch, n_epoch_init, step, n_step_epoch, time.time() - step_time, mse_loss))\n",
    "    if (epoch != 0) and (epoch % 10 == 0):\n",
    "        tl.vis.save_images(fake_hr_patchs.numpy(), [2, 4], os.path.join(save_dir, 'train_g_init_{}.png'.format(epoch)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.save_weights(os.path.join(checkpoint_dir, 'g-initial.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## adversarial learning (G, D)\n",
    "n_step_epoch = round(n_epoch // batch_size)\n",
    "for epoch in range(n_epoch):\n",
    "    for step, lr_patchs in enumerate(train_ds):\n",
    "        if lr_patchs.shape[0] != batch_size: # if the remaining data in this epoch < batch_size\n",
    "            break\n",
    "        step_time = time.time()\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            fake_patchs = G(lr_patchs)\n",
    "            fake_lr_patches = downscale_hr_patches(fake_patchs)\n",
    "            \n",
    "            logits_fake = D(fake_patchs)\n",
    "            logits_real = D(fake_lr_patches)\n",
    "            \n",
    "            feature_fake = VGG((fake_patchs+1)/2.) # the pre-trained VGG uses the input range of [0, 1]\n",
    "            feature_real = VGG((fake_lr_patches+1)/2.)\n",
    "            \n",
    "            d_loss1 = tl.cost.sigmoid_cross_entropy(logits_real, tf.ones_like(logits_real))\n",
    "            d_loss2 = tl.cost.sigmoid_cross_entropy(logits_fake, tf.zeros_like(logits_fake))\n",
    "            \n",
    "            d_loss = d_loss1 + d_loss2\n",
    "            g_gan_loss = 1e-3 * tl.cost.sigmoid_cross_entropy(logits_fake, tf.ones_like(logits_fake))\n",
    "\n",
    "            # mse_loss = tl.cost.mean_squared_error(fake_patchs, hr_patchs, is_mean=True)\n",
    "            \n",
    "            \n",
    "\n",
    "            mse_loss = tl.cost.mean_squared_error(fake_lr_patches, lr_patchs, is_mean=True)\n",
    "\n",
    "            vgg_loss = 2e-6 * tl.cost.mean_squared_error(feature_fake, feature_real, is_mean=True)\n",
    "            \n",
    "            g_loss = mse_loss + vgg_loss + g_gan_loss\n",
    "            \n",
    "            grad = tape.gradient(g_loss, G.trainable_weights)\n",
    "            g_optimizer.apply_gradients(zip(grad, G.trainable_weights))\n",
    "            grad = tape.gradient(d_loss, D.trainable_weights)\n",
    "            d_optimizer.apply_gradients(zip(grad, D.trainable_weights))\n",
    "        \n",
    "        with open(\"logs/gan.log\", \"a\") as log_file:\n",
    "            log_file.write(\"Epoch: [{}/{}] step: [{}/{}] time: {:.3f}s, g_loss(mse:{:.3f}, vgg:{:.3f}, adv:{:.3f}) d_loss: {:.3f}\".format(\n",
    "            epoch, n_epoch_init, step, n_step_epoch, time.time() - step_time, mse_loss, vgg_loss, g_gan_loss, d_loss))\n",
    "            log_file.close()\n",
    "            \n",
    "        print(\"Epoch: [{}/{}] step: [{}/{}] time: {:.3f}s, g_loss(mse:{:.3f}, vgg:{:.3f}, adv:{:.3f}) d_loss: {:.3f}\".format(\n",
    "            epoch, n_epoch_init, step, n_step_epoch, time.time() - step_time, mse_loss, vgg_loss, g_gan_loss, d_loss))\n",
    "\n",
    "    # update the learning rate\n",
    "    if epoch != 0 and (epoch % decay_every == 0):\n",
    "        new_lr_decay = lr_decay**(epoch // decay_every)\n",
    "        lr_v.assign(lr_init * new_lr_decay)\n",
    "        log = \" ** new learning rate: %f (for GAN)\" % (lr_init * new_lr_decay)\n",
    "        \n",
    "        with open(\"logs/learning_rate.log\", \"a\") as log_file:\n",
    "            log_file.write(log)\n",
    "            log_file.close()\n",
    "            \n",
    "        print(log)\n",
    "\n",
    "    if (epoch != 0) and (epoch % 10 == 0):\n",
    "        tl.vis.save_images(fake_patchs.numpy(), [2, 4], os.path.join(save_dir, 'train_g_{}.png'.format(epoch)))\n",
    "        G.save_weights(os.path.join(checkpoint_dir, 'g-{epoch}.h5'.format(epoch=epoch)))\n",
    "        D.save_weights(os.path.join(checkpoint_dir, 'd-{epoch}.h5'.format(epoch=epoch)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Conda-python3",
   "language": "python",
   "name": "conda-python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
